{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math, collections, itertools\n",
    "import nltk, nltk.classify.util\n",
    "from nltk import precision, recall\n",
    "from nltk.classify import NaiveBayesClassifier, MaxentClassifier, DecisionTreeClassifier\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_dict(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "def createFeatures(feature_select):\n",
    "    #reading pre-labeled input and splitting into lines\n",
    "    posSentences = open('positive.txt', 'r',encoding=\"utf8\")\n",
    "    negSentences = open('negative.txt', 'r',encoding=\"utf8\")\n",
    "    posSentences = re.split(r'\\n', posSentences.read())\n",
    "    negSentences = re.split(r'\\n', negSentences.read())\n",
    "\n",
    "    posFeatures = []\n",
    "    negFeatures = []\n",
    "    #http://stackoverflow.com/questions/367155/splitting-a-string-into-words-and-punctuation\n",
    "    #breaks up the sentences into lists of individual words (as selected by the input mechanism) and appends 'pos' or 'neg' after each list\n",
    "    for i in posSentences:\n",
    "        posWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        posWords = [feature_select(posWords), 'pos']\n",
    "        posFeatures.append(posWords)\n",
    "    for i in negSentences:\n",
    "        negWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        negWords = [feature_select(negWords), 'neg']\n",
    "        negFeatures.append(negWords)\n",
    "\n",
    "    #selects 3/4 of the features to be used for training and 1/4 to be used for testing\n",
    "    posCutoff = int(math.floor(len(posFeatures)*3/4))\n",
    "    negCutoff = int(math.floor(len(negFeatures)*3/4))\n",
    "    trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
    "    testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]\n",
    "    \n",
    "    return trainFeatures,testFeatures\n",
    "\n",
    "def evaluate(classifier,trainFeatures,testFeatures):\n",
    "    referenceSets = dict()\n",
    "    referenceSets['pos'] = set()\n",
    "    referenceSets['neg'] = set()\n",
    "    testSets = dict()\n",
    "    testSets['pos'] = set()\n",
    "    testSets['neg'] = set()\n",
    "    for i, (features, label) in enumerate(testFeatures):\n",
    "        referenceSets[label].add(i)\n",
    "        predicted = classifier.classify(features)\n",
    "        testSets[predicted].add(i)\n",
    "        \n",
    "    print ('train on %d instances, test on %d instances' % (len(trainFeatures), len(testFeatures)))\n",
    "    print ('accuracy:', nltk.classify.util.accuracy(classifier, testFeatures))\n",
    "    print ('pos precision:', precision(referenceSets['pos'], testSets['pos']))\n",
    "    print ('pos recall:', recall(referenceSets['pos'], testSets['pos']))\n",
    "    print ('neg precision:', precision(referenceSets['neg'], testSets['neg']))\n",
    "    print ('neg recall:', recall(referenceSets['neg'], testSets['neg']))\n",
    "    #classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_scores():\n",
    "    #splits sentences into lines                                                                                                  \n",
    "    posSentences = open('positive.txt', 'r')\n",
    "    negSentences = open('negative.txt', 'r')\n",
    "    posSentences = re.split(r'\\n', posSentences.read())\n",
    "    negSentences = re.split(r'\\n', negSentences.read())\n",
    "\n",
    "    #creates lists of all positive and negative words                                                                             \n",
    "    posWords = []\n",
    "    negWords = []\n",
    "    for i in posSentences:\n",
    "        posWord = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        posWords.append(posWord)\n",
    "    for i in negSentences:\n",
    "        negWord = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        negWords.append(negWord)\n",
    "    posWords = list(itertools.chain(*posWords))\n",
    "    negWords = list(itertools.chain(*negWords))\n",
    "\n",
    "    word_fd = FreqDist()\n",
    "    cond_word_fd = ConditionalFreqDist()\n",
    "\n",
    "    for word in posWords:\n",
    "        word_fd[word.lower()] +=1\n",
    "        cond_word_fd['pos'][word.lower()] +=1\n",
    "    for word in negWords:\n",
    "        word_fd[word.lower()] +=1\n",
    "        cond_word_fd['neg'][word.lower()] +=1\n",
    "        \n",
    "    pos_word_count = cond_word_fd['pos'].N()\n",
    "    neg_word_count = cond_word_fd['neg'].N()\n",
    "    total_word_count = pos_word_count + neg_word_count\n",
    "\n",
    "    word_scores = {}\n",
    "    for word, freq in word_fd.items():\n",
    "        pos_score = BigramAssocMeasures.chi_sq(cond_word_fd['pos'][word], (freq, pos_word_count), total_word_count)\n",
    "        neg_score = BigramAssocMeasures.chi_sq(cond_word_fd['neg'][word], (freq, neg_word_count), total_word_count)\n",
    "        word_scores[word] = pos_score + neg_score\n",
    "\n",
    "    return word_scores\n",
    "\n",
    "\n",
    "def find_best_words(word_scores, number):\n",
    "    best_vals = sorted(iter(word_scores.items()), key=lambda w_s: w_s[1], reverse=True)[:number]\n",
    "    best_words = set([w for w, s in best_vals])\n",
    "    return best_words\n",
    "\n",
    "def best_word_features(words):\n",
    "    return dict([(word, True) for word in words if word in best_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFeat = [10,100,1000,10000]\n",
    "word_scores = create_word_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features 10 done\n",
      "\n",
      "NaiveBayes 10 classifier\n",
      "\n",
      "train on 1183970 instances, test on 394657 instances\n",
      "accuracy: 0.5828985676169434\n",
      "pos precision: 0.5511850919320496\n",
      "pos recall: 0.8976132020552279\n",
      "neg precision: 0.7227492049566838\n",
      "neg recall: 0.26749259304354883\n",
      "NB Time used for 10 features is 2.46655\n",
      "Maximum Entropy 10 classifier\n",
      "\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.501\n",
      "             2          -0.64167        0.588\n",
      "             3          -0.62451        0.588\n",
      "             4          -0.61681        0.588\n",
      "         Final          -0.61287        0.588\n",
      "train on 1183970 instances, test on 394657 instances\n",
      "accuracy: 0.5829872522215493\n",
      "pos precision: 0.5514122461200659\n",
      "pos recall: 0.8949657040168063\n",
      "neg precision: 0.7197330919995137\n",
      "neg recall: 0.270323470920086\n",
      "MEnt Time used for 10 features is 261.87135\n",
      "SVM 10 classifier\n",
      "\n",
      "train on 1183970 instances, test on 394657 instances\n",
      "accuracy: 0.601491928434058\n",
      "pos precision: 0.6635370558375635\n",
      "pos recall: 0.41356652914525804\n",
      "neg precision: 0.5733578362771239\n",
      "neg recall: 0.7898301473274077\n",
      "SVM Time used for 10 features is 42.72096\n",
      "Features 100 done\n",
      "\n",
      "NaiveBayes 100 classifier\n",
      "\n",
      "train on 1183970 instances, test on 394657 instances\n",
      "accuracy: 0.688101313292302\n",
      "pos precision: 0.7127564725381494\n",
      "pos recall: 0.6313042597889088\n",
      "neg precision: 0.6684646792937224\n",
      "neg recall: 0.7450231340557653\n",
      "NB Time used for 100 features is 6.05007\n",
      "Maximum Entropy 100 classifier\n",
      "\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.501\n",
      "             2          -0.62413        0.696\n",
      "             3          -0.58485        0.698\n",
      "             4          -0.56064        0.699\n",
      "         Final          -0.54453        0.699\n",
      "train on 1183970 instances, test on 394657 instances\n",
      "accuracy: 0.6956749785256564\n",
      "pos precision: 0.6985921271957943\n",
      "pos recall: 0.6895036573945177\n",
      "neg precision: 0.692826594284913\n",
      "neg recall: 0.7018598563253379\n",
      "MEnt Time used for 100 features is 422.75050\n",
      "SVM 100 classifier\n",
      "\n",
      "train on 1183970 instances, test on 394657 instances\n",
      "accuracy: 0.6979554397869543\n",
      "pos precision: 0.6742901892383745\n",
      "pos recall: 0.7671264775114531\n",
      "neg precision: 0.7292571536188895\n",
      "neg recall: 0.6286324526157717\n",
      "SVM Time used for 100 features is 88.72105\n",
      "Features 1000 done\n",
      "\n",
      "NaiveBayes 1000 classifier\n",
      "\n",
      "train on 1183970 instances, test on 394657 instances\n",
      "accuracy: 0.7198504017412589\n",
      "pos precision: 0.8229737557367554\n",
      "pos recall: 0.5609861044319016\n",
      "neg precision: 0.6664410247731722\n",
      "neg recall: 0.8790636795324486\n",
      "NB Time used for 1000 features is 12.40451\n",
      "Maximum Entropy 1000 classifier\n",
      "\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.501\n",
      "             2          -0.62870        0.735\n",
      "             3          -0.58518        0.749\n",
      "             4          -0.55417        0.752\n",
      "         Final          -0.53123        0.754\n",
      "train on 1183970 instances, test on 394657 instances\n",
      "accuracy: 0.7532718284485008\n",
      "pos precision: 0.7816573504436921\n",
      "pos recall: 0.703632083829001\n",
      "neg precision: 0.7299924826247169\n",
      "neg recall: 0.803020617719875\n",
      "MEnt Time used for 1000 features is 674.11027\n",
      "SVM 1000 classifier\n",
      "\n",
      "train on 1183970 instances, test on 394657 instances\n",
      "accuracy: 0.766521308376641\n",
      "pos precision: 0.7528317365585465\n",
      "pos recall: 0.7943557164190438\n",
      "neg precision: 0.781844739442368\n",
      "neg recall: 0.7386257559154187\n",
      "SVM Time used for 1000 features is 197.08959\n",
      "Features 10000 done\n",
      "\n",
      "NaiveBayes 10000 classifier\n",
      "\n",
      "train on 1183970 instances, test on 394657 instances\n",
      "accuracy: 0.7296487836273017\n",
      "pos precision: 0.8489602138725215\n",
      "pos recall: 0.5594168417322636\n",
      "neg precision: 0.6709277617718896\n",
      "neg recall: 0.9002546775437315\n",
      "NB Time used for 10000 features is 17.98695\n",
      "Maximum Entropy 10000 classifier\n",
      "\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.501\n",
      "             2          -0.63370        0.750\n",
      "             3          -0.59082        0.766\n",
      "             4          -0.55873        0.770\n",
      "         Final          -0.53407        0.772\n",
      "train on 1183970 instances, test on 394657 instances\n",
      "accuracy: 0.7687308219542539\n",
      "pos precision: 0.804219505911316\n",
      "pos recall: 0.7110734263079298\n",
      "neg precision: 0.7405542019709808\n",
      "neg recall: 0.8265148747919965\n",
      "MEnt Time used for 10000 features is 868.40899\n",
      "SVM 10000 classifier\n",
      "\n",
      "train on 1183970 instances, test on 394657 instances\n",
      "accuracy: 0.7861434106071855\n",
      "pos precision: 0.7806133837294088\n",
      "pos recall: 0.7966488648156116\n",
      "neg precision: 0.7919183233706631\n",
      "neg recall: 0.7756148788506027\n",
      "SVM Time used for 10000 features is 217.97556\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "for num in nFeat:\n",
    "    best_words = find_best_words(word_scores, num)\n",
    "    trainFeatures10,testFeatures10 = createFeatures(best_word_features)\n",
    "    print('Features %d done\\n'%num)\n",
    "    print('NaiveBayes %d classifier\\n'%num)\n",
    "    nbstart = timeit.default_timer()\n",
    "    NBclassifier10 = NaiveBayesClassifier.train(trainFeatures10)\n",
    "    nbtime = timeit.default_timer()-nbstart\n",
    "    evaluate(NBclassifier10,trainFeatures10,testFeatures10)\n",
    "    print(\"NB Time used for %d features is %.5f\"%(num,nbtime))\n",
    "    print('Maximum Entropy %d classifier\\n'%num)\n",
    "    mentstart = timeit.default_timer()\n",
    "    MEclassifier10 = MaxentClassifier.train(trainFeatures10,max_iter=5)\n",
    "    menttime = timeit.default_timer()-mentstart\n",
    "    evaluate(MEclassifier10,trainFeatures10,testFeatures10)\n",
    "    print(\"MEnt Time used for %d features is %.5f\"%(num,menttime))\n",
    "    print('SVM %d classifier\\n'%num)\n",
    "    svmstart = timeit.default_timer()\n",
    "    SVMclassifier10 = nltk.classify.SklearnClassifier(LinearSVC()).train(trainFeatures10)\n",
    "    svmtime = timeit.default_timer()-svmstart\n",
    "    evaluate(SVMclassifier10,trainFeatures10,testFeatures10)\n",
    "    print(\"SVM Time used for %d features is %.5f\"%(num,svmtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures,testFeatures = createFeatures(make_full_dict)\n",
    "NBclassifier = NaiveBayesClassifier.train(trainFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes classifier\n",
      "train on 1183970 instances, test on 394657 instances\n",
      "accuracy: 0.7513004963804012\n",
      "pos precision: 0.8611595593151362\n",
      "pos recall: 0.5998582601432585\n",
      "neg precision: 0.6924914317280872\n",
      "neg recall: 0.9030754089045822\n",
      "Most Informative Features\n",
      "            tweeteradder = True              pos : neg    =    487.3 : 1.0\n",
      "                    Poem = True              pos : neg    =     75.5 : 1.0\n",
      "              Banksyart2 = True              pos : neg    =     56.9 : 1.0\n",
      "                    sadd = True              neg : pos    =     49.1 : 1.0\n",
      "                  Farrah = True              neg : pos    =     48.8 : 1.0\n",
      "                 saddens = True              neg : pos    =     48.4 : 1.0\n",
      "             shareholder = True              pos : neg    =     48.2 : 1.0\n",
      "                     447 = True              neg : pos    =     47.9 : 1.0\n",
      "                     SAD = True              neg : pos    =     47.0 : 1.0\n",
      "                 McMahon = True              neg : pos    =     46.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print ('NaiveBayes classifier')\n",
    "evaluate(NBclassifier,trainFeatures,testFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 10 classifier\n",
      "train on 1183970 instances, test on 394657 instances\n",
      "accuracy: 0.6014843268965203\n",
      "pos precision: 0.6625308771533283\n",
      "pos recall: 0.415464830798046\n",
      "neg precision: 0.573556294985948\n",
      "neg recall: 0.7879124558626568\n",
      "DT Time used for 10 features is 193.16014\n",
      "Decision Tree 20 classifier\n",
      "train on 1183970 instances, test on 394657 instances\n",
      "accuracy: 0.6162313097195793\n",
      "pos precision: 0.6951326909072434\n",
      "pos recall: 0.4155559492773798\n",
      "neg precision: 0.5825399637695571\n",
      "neg recall: 0.8173474978692317\n",
      "DT Time used for 20 features is 594.99130\n",
      "Decision Tree 30 classifier\n",
      "train on 1183970 instances, test on 394657 instances\n",
      "accuracy: 0.636360181119301\n",
      "pos precision: 0.7011046762643481\n",
      "pos recall: 0.47677744311422715\n",
      "neg precision: 0.6029486898766513\n",
      "neg recall: 0.7962934778197167\n",
      "DT Time used for 30 features is 1229.37088\n"
     ]
    }
   ],
   "source": [
    "nfeat = [10, 20, 30]\n",
    "for num in nfeat:\n",
    "    best_words = find_best_words(word_scores, num)\n",
    "    trainFeatures10,testFeatures10 = createFeatures(best_word_features)\n",
    "    print('Decision Tree %d classifier'%num)\n",
    "    dtstart = timeit.default_timer()\n",
    "    DTclassifier10 = DecisionTreeClassifier.train(trainFeatures10,entropy_cutoff=0.3)\n",
    "    dttime = timeit.default_timer()-dtstart\n",
    "    evaluate(DTclassifier10,trainFeatures10,testFeatures10)\n",
    "    print(\"DT Time used for %d features is %.5f\"%(num,dttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Let's tweak our network from before to model these phenomena\n",
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews,labels,hidden_nodes = 10, learning_rate = 0.1):\n",
    "       \n",
    "        # set our random number generator \n",
    "        np.random.seed(1)\n",
    "    \n",
    "        self.pre_process_data(reviews, labels)\n",
    "        \n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "        \n",
    "        \n",
    "    def pre_process_data(self, reviews, labels):\n",
    "        \n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                review_vocab.add(word)\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "        \n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = i\n",
    "        \n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = i\n",
    "         \n",
    "        \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "    \n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                                (self.hidden_nodes, self.output_nodes))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layer_0 = np.zeros((1,input_nodes))\n",
    "    \n",
    "        \n",
    "    def update_input_layer(self,review):\n",
    "\n",
    "        # clear out previous state, reset the layer to be all 0s\n",
    "        self.layer_0 *= 0\n",
    "        for word in review.split(\" \"):\n",
    "            if(word in self.word2index.keys()):\n",
    "                self.layer_0[0][self.word2index[word]] += 1\n",
    "                \n",
    "    def get_target_for_label(self,label):\n",
    "        if(label == 'POSITIVE'):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return output * (1 - output)\n",
    "    \n",
    "    def train(self, training_reviews, training_labels):\n",
    "        \n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        \n",
    "        correct_so_far = 0\n",
    "        perc = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(training_reviews)):\n",
    "            \n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            \n",
    "            #### Implement the forward pass here ####\n",
    "            ### Forward pass ###\n",
    "\n",
    "            # Input Layer\n",
    "            self.update_input_layer(review)\n",
    "\n",
    "            # Hidden layer\n",
    "            layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "\n",
    "            # Output layer\n",
    "            layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
    "\n",
    "            #### Implement the backward pass here ####\n",
    "            ### Backward pass ###\n",
    "\n",
    "            # TODO: Output error\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output.\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "\n",
    "            # TODO: Backpropagated error\n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer\n",
    "            layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error\n",
    "\n",
    "            # TODO: Update the weights\n",
    "            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step\n",
    "            self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate # update input-to-hidden weights with gradient descent step\n",
    "\n",
    "            if(np.abs(layer_2_error) < 0.5):\n",
    "                correct_so_far += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            if (i/len(training_reviews)>=(perc+1)*0.1):\n",
    "                sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "                perc +=1\n",
    "            if (i==len(training_reviews)-1):\n",
    "                sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "      \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \n",
    "        correct = 0\n",
    "        perc = 0\n",
    "        tt = 0\n",
    "        tf = 0\n",
    "        ft = 0\n",
    "        ff = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "            if(testing_labels[i]==\"POSITIVE\" and pred==\"POSITIVE\"):\n",
    "                tt +=1\n",
    "            if(testing_labels[i]==\"POSITIVE\" and pred==\"NEGATIVE\"):\n",
    "                tf +=1\n",
    "            if(testing_labels[i]==\"NEGATIVE\" and pred==\"POSITIVE\"):\n",
    "                ft +=1\n",
    "            if(testing_labels[i]==\"NEGATIVE\" and pred==\"NEGATIVE\"):\n",
    "                ff +=1\n",
    "                \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            if (i/len(testing_reviews)>=(perc+1)*0.1):\n",
    "                sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                                 + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                                + \"% #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "                perc +=1\n",
    "            if (i==len(testing_reviews)-1):\n",
    "                sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                                 + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                                + \"% #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "                pospre = tt/(tt+ft)\n",
    "                posrec = tt/(tt+tf)\n",
    "                negpre = ff/(ff+tf)\n",
    "                negrec = ff/(ff+ft)\n",
    "                print(\"pos precision: %.2f\"%pospre)\n",
    "                print(\"pos recall: %.2f\"%posrec)\n",
    "                print(\"neg precision: %.2f\"%negpre)\n",
    "                print(\"neg recall: %.2f\"%negrec)\n",
    "    def run(self, review):\n",
    "        \n",
    "        # Input Layer\n",
    "        self.update_input_layer(review.lower())\n",
    "\n",
    "        # Hidden layer\n",
    "        layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "\n",
    "        # Output layer\n",
    "        layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
    "        \n",
    "        if(layer_2[0] > 0.5):\n",
    "            return \"POSITIVE\"\n",
    "        else:\n",
    "            return \"NEGATIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertnnfeature(features):\n",
    "    featuresnn = []\n",
    "    labelsnn = []\n",
    "    for feat in features:\n",
    "        str = ''\n",
    "        for k in feat[0].keys():\n",
    "            str += k+' '\n",
    "        featuresnn.append(str)\n",
    "        if (feat[1]=='pos'):\n",
    "            labelsnn.append(\"POSITIVE\")\n",
    "        else:\n",
    "            labelsnn.append(\"NEGATIVE\")\n",
    "    return featuresnn,labelsnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features 10 done\n",
      "Neural Network 10\n",
      "Progress:99.9% Speed(reviews/sec):37181 #Correct:726554 #Trained:1183970 Training Accuracy:61.3%nn train feature 10 time is : 31.84232\n",
      " \n",
      "Features 100 done\n",
      "Neural Network 100\n",
      "Progress:99.9% Speed(reviews/sec):28997 #Correct:823843 #Trained:1183970 Training Accuracy:69.5%nn train feature 100 time is : 40.82929\n",
      " \n",
      "Features 1000 done\n",
      "Neural Network 1000\n",
      "Progress:99.9% Speed(reviews/sec):11539 #Correct:881036 #Trained:1183970 Training Accuracy:74.4%nn train feature 1000 time is : 102.59473\n",
      " \n",
      "Features 10000 done\n",
      "Neural Network 10000\n",
      "Progress:99.9% Speed(reviews/sec):1771. #Correct:887319 #Trained:1183970 Training Accuracy:74.9%nn train feature 10000 time is : 668.43656\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for num in nFeat:\n",
    "    best_words = find_best_words(word_scores, num)\n",
    "    trainFeatures10,testFeatures10 = createFeatures(best_word_features)\n",
    "    trainFeatures10,testFeatures10 = createFeatures(best_word_features)\n",
    "    trainnnfeatures, trainlabelsnn = convertnnfeature(trainFeatures10)\n",
    "    testnnfeatures,testlabelsnn = convertnnfeature(testFeatures10)\n",
    "    combined = list(zip(trainnnfeatures, trainlabelsnn))\n",
    "    np.random.shuffle(combined)\n",
    "    trainnnfeatures, trainlabelsnn = zip(*combined)\n",
    "    print('Features %d done'%num)\n",
    "    print('Neural Network %d'%num)\n",
    "    \n",
    "    mlp = SentimentNetwork(trainnnfeatures,trainlabelsnn,learning_rate=0.001)\n",
    "    \n",
    "    nnstart = timeit.default_timer()\n",
    "    mlp.train(trainnnfeatures,trainlabelsnn)\n",
    "    nntime = timeit.default_timer()-nnstart\n",
    "    print(\"nn train feature %d time is : %.5f\"%(num,nntime))\n",
    "    #mlp.test(testnnfeatures,testlabelsnn)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "708452"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
